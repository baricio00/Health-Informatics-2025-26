{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ciEBGMI5alog",
        "6vWzbwC_pr-7",
        "44FLh9uKSq-V",
        "Q4f4yPryPp8X"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baricio00/Health-Informatics-2025-26/blob/main/Copy_of_Lab_III_SMHD_25_26_TBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=\"6\">Statistical Models for Healthcare Data</font>**\n",
        "\n",
        "**<font size=\"5\">MSc in Health Informatics - UniSR - A.Y. 2025-2026</font>**\n",
        "\n",
        "Prof. Lara Cavinato - Dott. Vittorio Torri\n",
        "\n",
        "---\n",
        "\n",
        "<font size=\"4\">Lab III - Linear Regression</font>"
      ],
      "metadata": {
        "id": "v213oWQNY_4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "ciEBGMI5alog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "metadata": {
        "id": "V4tZOCOQanB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "XBmIG3NMcLCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "WcaQ5G3rJ5JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "B56Kp3nQectc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "1_MvRLcN_No-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "gEbCEGDUUmxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "metadata": {
        "id": "0qNen7Ljcx3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "6vWzbwC_pr-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same dataset as Labs I and II"
      ],
      "metadata": {
        "id": "up2s1UAuhw0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ias1u5HLpuR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/'\n",
        "df = pd.read_csv(PATH + 'heart_failure_clinical_records_dataset_smhd.csv')"
      ],
      "metadata": {
        "id": "KrcL7EH9YktP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "IXvXjHqHauKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_vars = ['anaemia', 'diabetes', 'high_blood_pressure',  'sex',  'smoking',  'DEATH_EVENT']\n",
        "num_vars = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'bmi', 'time']"
      ],
      "metadata": {
        "id": "qz4KsntemxVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression"
      ],
      "metadata": {
        "id": "44FLh9uKSq-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the scatterplot of bmi and ejection fraction"
      ],
      "metadata": {
        "id": "HTqg5ZdmUVwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(x=\"ejection_fraction\", y=\"bmi\",\n",
        "            height=6, data=df)"
      ],
      "metadata": {
        "id": "-aCiTJE6Ssoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there is a relationship between them: we can try to predict BMI based on ejection fraction"
      ],
      "metadata": {
        "id": "bYZ-Vkx8UaLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\epsilon $$"
      ],
      "metadata": {
        "id": "tGg_tPMXU2UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['ejection_fraction']]\n",
        "y = df['bmi']\n",
        "# Linear regression = certain numerical variable as a linear function of other variables"
      ],
      "metadata": {
        "id": "ZUgBUUYScph7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "8s5ms0ZfjXma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_const = sm.add_constant(X)"
      ],
      "metadata": {
        "id": "BVFQXNc0yCwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_const"
      ],
      "metadata": {
        "id": "TfawXKw90xS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the central part is the summary\n",
        "First thing to look at is standard error. The higher it is, the more uncertain it is"
      ],
      "metadata": {
        "id": "UYsioHI9wS_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y, X_const)\n",
        "results = model.fit() # will round the computation\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "Fx-4VWM8JKnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum{(y_i - \\hat{y_i})^2}}{\\sum{(y_i - \\bar{y})^2}}$$"
      ],
      "metadata": {
        "id": "tSv4fr6Z46TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = results.predict(X_const)\n",
        "\n",
        "r2 = r2_score(y,y_pred)\n",
        "mse = mean_squared_error(y,y_pred)\n",
        "\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}') # the mean of the square of the errors"
      ],
      "metadata": {
        "id": "retBsP9QeJxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_plot = np.arange(np.min(X),np.max(X),0.1).reshape(-1,1) # creates a grid of points on the x-axis (bmi) with step=0.1\n",
        "X_plot = sm.add_constant(X_plot)\n",
        "y_plot = results.predict(X_plot) # predict the model's output on this grid\n",
        "sns.relplot(x=\"ejection_fraction\", y=\"bmi\", palette=\"muted\", height=6, data=df)\n",
        "sns.lineplot(x=X_plot[:,1],y=y_plot,color='red') # our fitted model"
      ],
      "metadata": {
        "id": "Uk4s8icVc2Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the line corresponds to our model"
      ],
      "metadata": {
        "id": "ZtfPEvRQyhhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-test set"
      ],
      "metadata": {
        "id": "OTz5PITEP0Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we are evaluating the model on the same data it was trained on.."
      ],
      "metadata": {
        "id": "yaccy9TQN-UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X = df[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'ejection_fraction', 'time']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=1234)\n",
        "# in many case test_size can be reduced, but it's rare to see it higher, because the more the data, the more the model can learn\n",
        "\n",
        "train_index = X_train.index\n",
        "test_index = X_test.index\n",
        "\n",
        "# if you take all the data for whatever model, there is the risk of overfitting the model (\"learning by heart the data\")\n",
        "# If I run the model on other data, I see that the model performs better\n",
        "# If I see the model performs very bad on test data, there is need to give other training data"
      ],
      "metadata": {
        "id": "leZatsmLeGnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "B0dyUWHTsHq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "id": "pmuwmXdQsJP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: FIT THE MODEL ON TRAINING DATA\n",
        "X_train_1 = X_train[['ejection_fraction']]\n",
        "X_train_1 = sm.add_constant(X_train_1)\n",
        "\n",
        "model = sm.OLS(y_train, X_train_1)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "jZMbLOoxOF9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: COMPUTE R^2 AND MSE ON TEST DATA\n",
        "X_test_1 = X_test[['ejection_fraction']]\n",
        "X_test_1 = sm.add_constant(X_test_1)\n",
        "\n",
        "y_pred = results.predict(X_test_1)\n",
        "\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "VJIURJWQ1zpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R2 is **higher** than the one with training data, so the model is not overfitting (instead, it's **underfitting**). The model is not performing very well, so there will be other ways to improve it"
      ],
      "metadata": {
        "id": "4Qh9Hprc2dkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple linear regression"
      ],
      "metadata": {
        "id": "4Wvd8SYSsXwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple input variables"
      ],
      "metadata": {
        "id": "xstNp3L70osU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot age + \\beta_2 \\cdot \\text{ creatinine_phosphokinase } + ... + \\epsilon $$"
      ],
      "metadata": {
        "id": "2bjgHHzHLC0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = num_vars.copy()\n",
        "variables.remove('bmi')\n",
        "\n",
        "X_train_2 = X_train[variables]\n",
        "X_test_2 = X_test[variables]\n",
        "X_train_2 = sm.add_constant(X_train_2)\n",
        "X_test_2 = sm.add_constant(X_test_2)"
      ],
      "metadata": {
        "id": "XkRl80fx0UEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_2)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "Kr9FAuQ60Y0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a higher R2, interesting...\n",
        "\n",
        "When having very small coefficient, we have high P-values. If I have multiple variables and some of them have high P-value, I don't discard all of them (of course some will be useless), because they could be related in some way.\n",
        "\n",
        "There is also a warning... the [2], if the number is very high, there could be some issue in the model, implying that some of the coefficient are not very reliable in the estimation. Why this happens?\n",
        "- we have colinearity among the variables. There could be variables related to the others (how we can detect these and remove them?)\n",
        "- the numerical variables have different scale and can lead to problems in computation (How to solve this issue?)"
      ],
      "metadata": {
        "id": "BiwooJp_3R5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The condition number is a measure of how sensitive the solution of linear equations (like the ones for regression coefficients $\\beta$) are to small changes in the input data.\n",
        "\n",
        "In regression a high condition number implies near-linear dependencies or strongly correlated directions in feature space, even if not visible in pairwise correlations."
      ],
      "metadata": {
        "id": "95Offc3bf55C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum{(y_i - \\hat{y_i})^2}}{\\sum{(y_i - \\bar{y})^2}}$$\n",
        "\n",
        "$$ AdjR^2 = 1 - \\frac{n - 1}{n - p -1} \\frac{RSS}{TSS}$$\n",
        "\n",
        "where $p$ is the number of regressors"
      ],
      "metadata": {
        "id": "XsJxdcduPybA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_2)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_2.shape[1] # num of input variables in the model\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) # adjusted R2\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "m5mtIr-z0mCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the adjusted R2 is significantly lower. Now we have models performing good on test data, but ..."
      ],
      "metadata": {
        "id": "n5KbPgQN49Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collinearity"
      ],
      "metadata": {
        "id": "ls8_Nc5_wShq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Variance Inflation Factor (VIF) indicates how good a variable can be predicted from the others\n",
        "\n",
        "$$\n",
        "VIF_j = \\frac{1}{1 - R^2_j}\n",
        "$$\n",
        "\n",
        "where $R^2_j$ is the $R^2$ of the linear regression model predicting varibale $j$ from all other variables\n",
        "\n",
        "A high VIF means there is collinearity with some other variable\n",
        "\n",
        "Rule of thumb: VIF > 5 for a variable is problematic"
      ],
      "metadata": {
        "id": "jEqhQqPJ5Fjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X_train_2.columns\n",
        "\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_2.values, i) for i in range(X_train_2.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "8AjN-zEU5Hq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding this variable doesn't add anything useful to the model"
      ],
      "metadata": {
        "id": "0QCDs7rT5Om8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficients can also identify collinearity, but not always. If a variable X1 is a linear transformation of a variable X2, their linear correlation coefficient will be high. But if the collinearity involves a group of variable, then it might not be evident from the correlation matrix"
      ],
      "metadata": {
        "id": "zZiGkeHg5MQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df[num_vars].corr(), cmap=\"Blues\",annot=True);"
      ],
      "metadata": {
        "id": "t-CdubuGwUfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale variables"
      ],
      "metadata": {
        "id": "C4D-4YA218jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical variables having different scales are often problematic. Many models have their coefficients' computation affected by this, but while this is not the case of linear regression with least square method, it is still useful to **standardize** the features for interpretability of coefficients and to reduce the condition numer when it is not due to a direct multicollinearity among the variables. The VIF is scale invariant, but the condition number is not.\n",
        "\n",
        "Moreover, this is necessary when penalizations are introduced in linear regression models (more on this in the next lectures)\n",
        "\n",
        "To standardize a variable x:\n",
        "\n",
        "$$ z = \\frac{x - \\mu }{\\sigma}$$"
      ],
      "metadata": {
        "id": "279luzPIP4Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ $$"
      ],
      "metadata": {
        "id": "kGID2zcfPisq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mu and sigma have to be estimated"
      ],
      "metadata": {
        "id": "lV2fmTP16tYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2 = X_train[variables]\n",
        "X_test_2 = X_test[variables]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_2_scaled = pd.DataFrame(scaler.fit_transform(X_train_2), columns=X_train_2.columns, index=X_train_2.index)\n",
        "X_test_2_scaled = pd.DataFrame(scaler.transform(X_test_2), columns=X_test_2.columns, index=X_test_2.index)\n",
        "\n",
        "X_train_2_scaled = sm.add_constant(X_train_2_scaled)\n",
        "X_test_2_scaled = sm.add_constant(X_test_2_scaled)"
      ],
      "metadata": {
        "id": "9CBYB412pyfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block handles the scaling of the numerical variables `ejection_fraction` and `serum_creatinine` before fitting the model. First, it extracts these variables for both the training and testing sets. Then, it initializes a `StandardScaler` object, which will standardize features by removing the mean and scaling to unit variance. It fits the scaler on the training data (`X_train_7`) and transforms both the training and testing data. This is crucial to prevent data leakage from the test set. The `[:, 1:]` slicing is used because the `PolynomialFeatures` function adds a constant term (column 0) that should not be scaled. Finally, it converts the scaled arrays back into pandas DataFrames, preserving column names and indices, and then explicitly adds a constant column to both the training and testing sets using `sm.add_constant` for the OLS model"
      ],
      "metadata": {
        "id": "AHcyEipX-57M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_2_scaled)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "6lzN9qrFp5GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_2_scaled)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_2_scaled.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "sV4AiGB5qAef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce the number of variables"
      ],
      "metadata": {
        "id": "2EEmvyUWdPbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EF + creatinine"
      ],
      "metadata": {
        "id": "r1zRIwzr15Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\epsilon $$"
      ],
      "metadata": {
        "id": "DnO172BVPShL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['ejection_fraction', 'serum_creatinine']\n",
        "\n",
        "X_train_3 = X_train[variables]\n",
        "X_test_3 = X_test[variables]\n",
        "X_train_3 = sm.add_constant(X_train_3)\n",
        "X_test_3 = sm.add_constant(X_test_3)"
      ],
      "metadata": {
        "id": "Dqhp2j93dSet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_3)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "lVsX0ItZddjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_3)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_3.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "PTvDsoKSdhSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with 2 regressors we can visualize a plane:"
      ],
      "metadata": {
        "id": "Km2_7Z-yCSFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = X_train_3['ejection_fraction']\n",
        "x2 = X_train_3['serum_creatinine']\n",
        "y = y_train\n",
        "\n",
        "# Get model coefficients\n",
        "b0, b1, b2 = results.params\n",
        "\n",
        "# Create a grid for plotting the regression plane\n",
        "x1_grid, x2_grid = np.meshgrid(\n",
        "    np.linspace(x1.min(), x1.max(), 30),\n",
        "    np.linspace(x2.min(), x2.max(), 30)\n",
        ")\n",
        "\n",
        "# Compute predicted y values over the grid\n",
        "y_pred_grid = b0 + b1 * x1_grid + b2 * x2_grid\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "views = [\n",
        "    (0, 45),\n",
        "    (20, 135),\n",
        "    (0, 0),\n",
        "    (10, 220)\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    \"Regression Plane View 1\",\n",
        "    \"Regression Plane View 2\",\n",
        "    \"Regression Plane View 3\",\n",
        "    \"Regression Plane View 4\"\n",
        "]\n",
        "\n",
        "for i, (elev, azim) in enumerate(views):\n",
        "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
        "    ax.scatter(x1, x2, y, color='r', label='Data')\n",
        "    ax.plot_surface(x1_grid, x2_grid, y_pred_grid, alpha=0.4)\n",
        "    ax.set_xlabel('Ejection Fraction')\n",
        "    ax.set_ylabel('Serum Creatinine')\n",
        "    ax.set_zlabel('Target')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_title(titles[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLp4NMT-CQNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EF + creatinine + time"
      ],
      "metadata": {
        "id": "-MepDiXY1_Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\beta_3 \\cdot time + \\epsilon $$"
      ],
      "metadata": {
        "id": "sVaMlpZbSZTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "variables = ['ejection_fraction', 'serum_creatinine', 'time']\n",
        "\n",
        "X_train_4 = X_train[variables]\n",
        "X_test_4 = X_test[variables]\n",
        "\n",
        "# scaler =\n",
        "\n",
        "X_train_4 = sm.add_constant(X_train_4)\n",
        "X_test_4 = sm.add_constant(X_test_4)"
      ],
      "metadata": {
        "id": "tJPjLRSsDY7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_4)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "a_yqroILDlvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_4)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_4.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "I2h0fltzDlvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial features"
      ],
      "metadata": {
        "id": "Q4f4yPryPp8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot ef^2 + \\epsilon $$"
      ],
      "metadata": {
        "id": "oMFR-C3WSq2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is linear in coefficients. Why don't you include the $ef$ and the $ef^2$?\n",
        "\n",
        "We create the object, specify the degree (square, cube, etc.) and create the fit_transform method, scale everything (but why? we have only one variable... We now have 2 variables!)\n"
      ],
      "metadata": {
        "id": "vxDj5GO-bnk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "ZCymbOcNsZMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['ejection_fraction']\n",
        "\n",
        "X_train_6 = X_train[variables]\n",
        "X_test_6 = X_test[variables]\n",
        "\n",
        "polynomial2 = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_6 = polynomial2.fit_transform(X_train_6)\n",
        "X_test_6 = polynomial2.fit_transform(X_test_6)"
      ],
      "metadata": {
        "id": "GK3xP1RaHur3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale everything except the constant term (column index 0)\n",
        "X_train_6[:, 1:] = scaler.fit_transform(X_train_6[:, 1:])\n",
        "X_test_6[:, 1:] = scaler.transform(X_test_6[:, 1:])\n",
        "\n",
        "# Convert to DataFrame, maintaining the column names\n",
        "cols = ['const', 'ejection_fraction', 'ejection_fraction^2']\n",
        "#cols = polynomial2.get_feature_names_out()\n",
        "X_train_6 = pd.DataFrame(X_train_6, columns=cols, index=train_index)\n",
        "X_test_6 = pd.DataFrame(X_test_6, columns=cols, index=test_index)"
      ],
      "metadata": {
        "id": "G_7fWJOM6BCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block continues the preparation of the polynomial features for the model. First, it initializes a `StandardScaler`. Then, it applies this scaler to both the training (`X_train_6`) and testing (`X_test_6`) polynomial features.\n",
        "\n",
        "It's important to note that the scaling is applied to all columns except the first one (`[:, 1:]`), because the first column typically represents the constant term (bias) which should not be scaled. The `fit_transform` method is used on the training data to learn the scaling parameters (mean and standard deviation) and then apply the transformation.\n",
        "\n",
        "For the test data, only the `transform` method is used, applying the scaling parameters learned from the training data, which prevents data leakage.\n",
        "\n",
        "Finally, both the scaled training and testing feature arrays are converted back into pandas DataFrames, assigning appropriate column names (`'const'`, `'ejection_fraction'`, `'ejection_fraction^2'`) and preserving their original indices."
      ],
      "metadata": {
        "id": "Grfd2ur_Atn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_6"
      ],
      "metadata": {
        "id": "F-aiQCCI6BCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_6)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "6Mchp_ON6BCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_6)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_6.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "vSGX6MMJ6BCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the quadratic and the linear models on bmi graphically"
      ],
      "metadata": {
        "id": "0qFHoSnZJpQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_plot = np.arange(np.min(X['ejection_fraction']),np.max(X['ejection_fraction']),0.1).reshape(-1,1)\n",
        "X_plot_6 = polynomial2.fit_transform(X_plot)\n",
        "\n",
        "X_plot_6_scaled = X_plot_6\n",
        "X_plot_6_scaled[:,1:] = scaler.transform(X_plot_6_scaled[:,1:])\n",
        "y_plot_6 = results.predict(X_plot_6_scaled)\n",
        "\n",
        "sns.relplot(y=\"bmi\", x=\"ejection_fraction\", height=6, data=df)\n",
        "sns.lineplot(x=X_plot[:,0], y=y_plot_6,color='red')\n",
        "sns.lineplot(x=X_plot[:,0], y=y_plot,color='green')"
      ],
      "metadata": {
        "id": "oGFfUNlws9C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\beta_3 \\cdot ef^2 + \\beta_4 \\cdot ef \\cdot creatinine + \\beta_5 \\cdot creatinine^2 + \\epsilon $$"
      ],
      "metadata": {
        "id": "SfrgLIV5Rrn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It could be useful to have \"interaction terms\", because sometimes the target variable doesn't depend on one or another, but on the combination of them. High ef and low creatinine, taking the product could compensate."
      ],
      "metadata": {
        "id": "hJnt1lF5dmyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['ejection_fraction', 'serum_creatinine']\n",
        "\n",
        "X_train_7 = X_train[variables]\n",
        "X_test_7 = X_test[variables]\n",
        "\n",
        "polynomial2 = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_7 = polynomial2.fit_transform(X_train_7)\n",
        "X_test_7 = polynomial2.fit_transform(X_test_7)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale everything except the constant term (column index 0)\n",
        "X_train_7[:, 1:] = scaler.fit_transform(X_train_7[:, 1:])\n",
        "X_test_7[:, 1:] = scaler.transform(X_test_7[:, 1:])\n",
        "\n",
        "# Convert to DataFrame, maintaining the column names\n",
        "cols = polynomial2.get_feature_names_out()\n",
        "X_train_7 = pd.DataFrame(X_train_7, columns=cols, index=train_index)\n",
        "X_test_7 = pd.DataFrame(X_test_7, columns=cols, index=test_index)"
      ],
      "metadata": {
        "id": "Y5kq8Q5hOwGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_7"
      ],
      "metadata": {
        "id": "Osng1tly6U90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_7)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "TXNkiFtqsfv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_7)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_7.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "w_Xc9Siosn6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\beta_3 \\cdot ef \\cdot creatinine + \\beta_4 \\cdot creatinine^2 + \\epsilon $$"
      ],
      "metadata": {
        "id": "adyBL0d5SCXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_8 = X_train_7.drop('ejection_fraction^2', axis=1)\n",
        "X_test_8 = X_test_7.drop('ejection_fraction^2', axis=1)"
      ],
      "metadata": {
        "id": "P_2amWIE4Y3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_8"
      ],
      "metadata": {
        "id": "s4HkOSVb4fbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the test set could not be so representative of the population. Maybe it's missing some data, or maybe we should cross-validate (next time)."
      ],
      "metadata": {
        "id": "mLMYqZxTfJZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_8)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "VVRlel6d4kf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_8)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_8.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "RXHruxVb4m-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Features"
      ],
      "metadata": {
        "id": "wy6V5Tko0y0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use them we need to transform categorical variables with one-hot encoding\n"
      ],
      "metadata": {
        "id": "ppFgrjXv09xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "OlLlwgM5Li2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a given cat variable, how many values/cat are there in the cat variables? (0/1, true/false, male/female/diverse, ...). It creates $k-1$ columns, where each of these referes to a specific value of the cat values. Each column will be binary (male column, female column) and mutually exclusive"
      ],
      "metadata": {
        "id": "Vb_OQyJmfvg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False) # drop the first value\n",
        "encoded_feature = encoder.fit_transform(df[cat_vars])\n",
        "\n",
        "df_encoded = pd.DataFrame(encoded_feature, columns=encoder.get_feature_names_out(cat_vars), index=df.index) # get_feature_names_out include the suffix in the column names\n",
        "\n",
        "df_encoded = pd.concat([df.drop(cat_vars, axis=1), df_encoded], axis=1) # concatenate the numerical variables with the one-hot encoded categorical variables\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "QWFQ-FGTl8vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot DEATH\\_EVENT\\_True + \\epsilon $$"
      ],
      "metadata": {
        "id": "CrF9oNLcScLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_variables = ['ejection_fraction']\n",
        "cat_variables = ['DEATH_EVENT_True'] # write the coded value\n",
        "\n",
        "# scaling categorical values doesn't make sense, so we don't do that\n",
        "\n",
        "X_train_9_num = X_train[num_variables]\n",
        "X_test_9_num = X_test[num_variables]\n",
        "\n",
        "X_train_9_cat = df_encoded.loc[train_index, cat_variables]\n",
        "X_test_9_cat = df_encoded.loc[test_index, cat_variables]\n",
        "\n",
        "X_train_9 = np.concatenate([X_train_9_num, X_train_9_cat], axis=1)\n",
        "X_test_9 = np.concatenate([X_test_9_num, X_test_9_cat], axis=1)\n",
        "\n",
        "cols = num_variables + cat_variables\n",
        "X_train_9 = pd.DataFrame(X_train_9, index=train_index, columns=cols)\n",
        "X_test_9 = pd.DataFrame(X_test_9, index=test_index, columns=cols)\n",
        "\n",
        "X_train_9 = sm.add_constant(X_train_9)\n",
        "X_test_9 = sm.add_constant(X_test_9)"
      ],
      "metadata": {
        "id": "gDNtBHXYCw_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_9)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "fwg3LS4jDOjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_plot = np.arange(np.min(df['ejection_fraction']), np.max(df['ejection_fraction']), 0.1).reshape(-1, 1)\n",
        "X_plot = sm.add_constant(X_plot)\n",
        "\n",
        "# Create two versions of X_plot, one with the binary variable = 0, one with binary variable = 1\n",
        "X_plot_0 = np.hstack([X_plot, np.zeros((X_plot.shape[0], 1))])  # Add column of 0's for binary_var = 0\n",
        "X_plot_1 = np.hstack([X_plot, np.ones((X_plot.shape[0], 1))])   # Add column of 1's for binary_var = 1\n",
        "\n",
        "y_plot_0 = results.predict(X_plot_0)  # Prediction for binary_var = 0\n",
        "y_plot_1 = results.predict(X_plot_1)  # Prediction for binary_var = 1\n",
        "\n",
        "sns.relplot(x=\"ejection_fraction\", y=\"bmi\", height=6, data=df, legend=False)\n",
        "\n",
        "sns.lineplot(x=X_plot[:, 1], y=y_plot_0, color='green', label='DEATH_EVENT_False')\n",
        "sns.lineplot(x=X_plot[:, 1], y=y_plot_1, color='red', label='DEATH_EVENT_True')"
      ],
      "metadata": {
        "id": "QsDfTltwDdxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_9)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_9.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "bsY46KWLDWjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\beta_3 \\cdot ef \\cdot creatinine + \\beta_4 \\cdot creatinine^2 + \\beta_5 \\cdot DEATH\\_EVENT\\_True + \\epsilon $$"
      ],
      "metadata": {
        "id": "F7BAqzCYUPt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_variables = ['ejection_fraction', 'serum_creatinine']\n",
        "cat_variables = ['DEATH_EVENT_True']\n",
        "\n",
        "X_train_9_num = X_train[num_variables]\n",
        "X_test_9_num = X_test[num_variables]\n",
        "\n",
        "X_train_9_cat = df_encoded.loc[train_index, cat_variables]\n",
        "X_test_9_cat = df_encoded.loc[test_index, cat_variables]\n",
        "\n",
        "# Perform Polynomial Features transformation on the numerical data\n",
        "polynomial2 = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_9_poly = polynomial2.fit_transform(X_train_9_num)\n",
        "X_test_9_poly = polynomial2.fit_transform(X_test_9_num)\n",
        "\n",
        "# Scale the polynomial features except for the bias (constant) term\n",
        "scaler = StandardScaler()\n",
        "X_train_9_poly[:, 1:] = scaler.fit_transform(X_train_9_poly[:, 1:])\n",
        "X_test_9_poly[:, 1:] = scaler.transform(X_test_9_poly[:, 1:])\n",
        "\n",
        "# Concatenate the polynomial features with the one-hot encoded categorical variable\n",
        "X_train_9 = np.concatenate([X_train_9_poly, X_train_9_cat], axis=1)\n",
        "X_test_9 = np.concatenate([X_test_9_poly, X_test_9_cat], axis=1)\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "cols_poly = polynomial2.get_feature_names_out(num_variables)\n",
        "cols = np.concatenate([cols_poly, X_train_9_cat.columns])\n",
        "\n",
        "X_train_9 = pd.DataFrame(X_train_9, columns=cols, index=train_index)\n",
        "X_test_9 = pd.DataFrame(X_test_9, columns=cols, index=test_index)\n",
        "\n",
        "X_train_9.drop('ejection_fraction^2', axis=1, inplace=True)\n",
        "X_test_9.drop('ejection_fraction^2', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "NW4QAoZZ5lME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_9"
      ],
      "metadata": {
        "id": "i8e8aIN96B4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_9)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "obXc6JsG5lME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_9)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_9.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "jokNsdBi5lME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnostics"
      ],
      "metadata": {
        "id": "WEvYXx5P_iOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the assumptions of linear regression.\n",
        "\n",
        "$$e_i=y_i-\\hat y_i$$\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "1. There is a linear relationship between the predictors and the response\n",
        "2. $\\epsilon_i$ (residuals) are indipendent\n",
        "3. $Var(\\epsilon_i) = \\sigma^2 \\quad \\forall i $ (homoschedasticity)\n",
        "\n",
        "and for inference:\n",
        "\n",
        "4. $\\epsilon_i \\sim N(0,\\sigma^2) \\quad \\forall i$ (normality) --- or at least large $n$ (sample size). The resifual should follow a normal distribution"
      ],
      "metadata": {
        "id": "k-uCFe1J_vV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider the model\n",
        "\n",
        "$$ bmi = \\beta_0 + \\beta_1 \\cdot ef + \\beta_2 \\cdot creatinine + \\beta_3 \\cdot \\text{creatinine_phosphokinase} + \\epsilon $$"
      ],
      "metadata": {
        "id": "Pkg_HjfBO0_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the math behind linear regression (p-values, std errors, conf intervals of the coefficients, ...) OLS relies on some assumptions, so we need to check if they're verified, because often they are not and we need some implications. If the assumptions are not validated, I cannot fully rely on the chosen predictors. I cannot say that a predictor is usful just because the p-value is 0."
      ],
      "metadata": {
        "id": "29FB2if2jC1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['ejection_fraction', 'serum_creatinine', 'creatinine_phosphokinase']\n",
        "\n",
        "X_train_5 = X_train[variables]\n",
        "X_test_5 = X_test[variables]\n",
        "X_train_5 = sm.add_constant(X_train_5)\n",
        "X_test_5 = sm.add_constant(X_test_5)"
      ],
      "metadata": {
        "id": "Ahzbj6svOz9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the assumptions\n",
        "1. let's come back later\n",
        "2. how do we verify this? in the 3rd table there is a reference test called \"Durbin-Watson\" that is useful"
      ],
      "metadata": {
        "id": "6Yz_fKpUk3k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(y_train, X_train_5)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "BRiIa3KX8t5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Independence"
      ],
      "metadata": {
        "id": "Tot0-zOz_8hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results summary includes Durbin-Watson test. The values of its test statistics are between 0 and 4. Values near 2 indicates no autocorrelation (independence), smaller values indicate positive autocorrelation, higher values negative autocorrelation"
      ],
      "metadata": {
        "id": "p0-qUUAnAINj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.stattools import durbin_watson"
      ],
      "metadata": {
        "id": "T58pFRlOCTzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If near 2, the residual could be considered independent"
      ],
      "metadata": {
        "id": "PZmoxk1hlWzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "durbin_watson(results.resid)"
      ],
      "metadata": {
        "id": "MPgJZpUA__HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linearity & Homoschedasticity"
      ],
      "metadata": {
        "id": "wkOqY3YVEfsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visually inspect the residuals"
      ],
      "metadata": {
        "id": "X2_fa4JAEiZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both of them can be understood by visualizing the residuals but also the fitted values. We could do a scatter plot to visualize them.\n",
        "\n",
        "Homosc means that the pred shoud have same variances"
      ],
      "metadata": {
        "id": "Rl1WCtW1ldBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_vals = results.fittedvalues\n",
        "residuals = results.resid\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.residplot(x=fitted_vals, y=residuals,\n",
        "              lowess=True, # Add a smooth trendline\n",
        "              line_kws={'color': 'red', 'lw': 1.5})\n",
        "plt.axhline(0, color='black', linestyle='--', lw=2)  # Horizontal line at 0\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted Values for Linearity & Homoscedasticity Check')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zKaQI_8yElrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the lowess=True is a smooth approx of the behavior that is automatically computed."
      ],
      "metadata": {
        "id": "GS273M2NlmHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breusch-Pagan statistical test for homoschedascity.\n",
        "\n",
        "*H0: residuals are homoschedastic*"
      ],
      "metadata": {
        "id": "PEhJdDSCHo5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "bp_test = het_breuschpagan(results.resid, X_train_5) # It's not very used, as it is very conservative (very easy to reject and say that there's no homoschedasticity)\n",
        "\n",
        "print(f'p-value: {bp_test[1]}')"
      ],
      "metadata": {
        "id": "u2E7nhyiEwv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussianity"
      ],
      "metadata": {
        "id": "VA0zjEzv_7SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results summary includes the Jacques-Bera test for normality on residuals.\n",
        "\n",
        "*H0: data are gaussian.*\n",
        "\n",
        "This test has poor power for n < 100.\n",
        "\n",
        "Other methods we previosly saw are the QQ-plot (graphical) and the Shapiro-Wilk test (good power for n < 2000)"
      ],
      "metadata": {
        "id": "sZZDml13Caz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very low p-value in JB test, I should reject the null hypothesis"
      ],
      "metadata": {
        "id": "ePgGZw5_nKzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Jacques-Bera test p-value: {results.diagn[\"jbpv\"]:.4f}')"
      ],
      "metadata": {
        "id": "nGFEXtNnDGOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stats.shapiro(results.resid).pvalue\n",
        "print(f'Shapiro-Wilk test p-value: {sw:.4f}')"
      ],
      "metadata": {
        "id": "bQTiXM49D3yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.probplot(results.resid, dist=\"norm\", plot=plt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G90j7E7gARxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform y"
      ],
      "metadata": {
        "id": "DRZALpmTG5Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_10 = np.sqrt(y_train)\n",
        "y_test_10 = np.sqrt(y_test)\n",
        "\n",
        "# y_train_10 = np.log(y_train + 1)\n",
        "# y_test_10 = np.log(y_test + 1)\n",
        "\n",
        "# y_train_10 = np.cbrt(y_train)\n",
        "# y_test_10 = np.cbrt(y_test)\n",
        "\n",
        "model = sm.OLS(y_train_10, X_train_5)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "fObpP6PUHeCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_5)\n",
        "r2 = r2_score(y_test_10,y_pred)\n",
        "mse = mean_squared_error(y_test_10,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_5.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "uK_pvqewHeCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stats.shapiro(results.resid).pvalue\n",
        "print(f'Shapiro-Wilk test p-value: {sw:.4f}')"
      ],
      "metadata": {
        "id": "s6HKoFjGHnWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.probplot(results.resid, dist=\"norm\", plot=plt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ktH04vCfHnWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leverages & outliers"
      ],
      "metadata": {
        "id": "tzMb70iTB4he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outliers**: points with a very high residual (bad fit)\n",
        "Residuals are standardized so to make it easier to spot outliers (those outside +- 2 or +- 3)\n",
        "\n",
        "**Leverages**: points with unusual value\n",
        "\n",
        "Hat Matrix $H$ s.t. $\\hat{y}=Hy$\n",
        "\n",
        "$H = X(X^TX)^{-1}X^T$\n",
        "\n",
        "the leverage of point $i$ is $h_{ii} = x_i^T(X^TX)^{-1}x_i \\in (0,1)$\n",
        "\n",
        "\n",
        "\n",
        "**Cook's distance**: a measure of the point's overall influence of the model\n",
        "\n",
        "$$\n",
        "D_i = \\frac{\\epsilon_i^2}{k \\cdot \\sigma^2} \\cdot \\frac{h_{ii}}{(1-h_{ii})^2}\n",
        "$$\n",
        "\n",
        "combining the size of the (standardized) residuals, with respect to the model parameters, and the leverage"
      ],
      "metadata": {
        "id": "l0fVpJLfPfjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are points with high Cook's distance, I should look into the data to see what's happening"
      ],
      "metadata": {
        "id": "bvITPa3HpWDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sm.graphics.influence_plot(results, criterion=\"cooks\", ax=ax);"
      ],
      "metadata": {
        "id": "eAq3YGPhB6nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jmh8pLn5p6Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigger is the point, the higher is the Cook's distance. The point in the right bottom angle has very low residual, high leverage and high Cook's distance"
      ],
      "metadata": {
        "id": "9fHNZn--pk-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "influence = results.get_influence()\n",
        "\n",
        "influence_df = influence.summary_frame()[['cooks_d', 'student_resid', 'hat_diag']]\n",
        "\n",
        "influence_df = influence_df.rename(columns={'hat_diag': 'leverage'}).sort_values(by='cooks_d', ascending=False)\n",
        "\n",
        "influence_df"
      ],
      "metadata": {
        "id": "ffXW4UJKNOWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point 9 has a significant influence on the model"
      ],
      "metadata": {
        "id": "jIt8THarqIpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.loc[9]"
      ],
      "metadata": {
        "id": "CVSJztZ6qN5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing (high leverage) outliers"
      ],
      "metadata": {
        "id": "f94bDXCKGrqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_remove = [9]\n",
        "\n",
        "X_train_11 = X_train_5.drop(to_remove)\n",
        "y_train_11 = y_train.drop(to_remove)\n",
        "X_test_11 = X_test_5\n",
        "y_test_11 = y_test\n",
        "model = sm.OLS(y_train_11, X_train_11)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "5Tjgm2YgFySX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the Prob(JB) completely changes"
      ],
      "metadata": {
        "id": "bXUGgGY3qnds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = results.predict(X_test_11)\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mse = mean_squared_error(y_test,y_pred)\n",
        "n = len(y_test)\n",
        "p = X_test_11.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f'R2: {r2:.4f}')\n",
        "print(f'adj_R2: {adj_r2:.4f}')\n",
        "print(f'MSE: {mse:.4f}')"
      ],
      "metadata": {
        "id": "kxclfooaGErq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stats.shapiro(results.resid).pvalue\n",
        "print(f'Shapiro-Wilk test p-value: {sw:.4f}')"
      ],
      "metadata": {
        "id": "f_0sRGDuQKud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.probplot(results.resid, dist=\"norm\", plot=plt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucuykCRPQMHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_vals = results.fittedvalues\n",
        "residuals = results.resid\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.residplot(x=fitted_vals, y=residuals, lowess=True,\n",
        "              line_kws={'color': 'red', 'lw': 1.5})  # Add a smooth trendline\n",
        "\n",
        "plt.axhline(0, color='black', linestyle='--', lw=2)  # Horizontal line at 0\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted Values for Linearity & Homoscedasticit Check')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sQURlwYFGYd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "bp_test = het_breuschpagan(results.resid, X_train_11)\n",
        "\n",
        "print(f'p-value: {bp_test[1]}')"
      ],
      "metadata": {
        "id": "A61W8qd1GcWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz\n"
      ],
      "metadata": {
        "id": "lkBpCQo2kA3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anscombe dataset"
      ],
      "metadata": {
        "id": "4S5aFSD7rH0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four datasets with nearly identical:\n",
        "\n",
        "* Mean of\n",
        "$x$ and $y$\n",
        "\n",
        "* Variance of\n",
        "$x$ and $y$\n",
        "\n",
        "* Correlation between\n",
        "$x$ and $y$\n",
        "\n",
        "* Regression line ($y$ = 3.00 + 0.50$x$ )\n",
        "\n",
        "* $^2$\n",
        "  and standard error"
      ],
      "metadata": {
        "id": "hGV08NFfkCOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the built-in Anscombe dataset\n",
        "df = sns.load_dataset(\"anscombe\")\n",
        "\n",
        "datasets = ['I', 'II', 'III', 'IV']\n",
        "models = {}\n",
        "\n",
        "# Plot setup\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "fig.suptitle(\"Anscombe's Quartet: Regression & Residuals\", fontsize=16)\n",
        "\n",
        "for i, name in enumerate(datasets):\n",
        "    data = df[df['dataset'] == name]\n",
        "\n",
        "    x = data['x']\n",
        "    y = data['y']\n",
        "    X = sm.add_constant(x)\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    models[name] = model\n",
        "\n",
        "    # Scatter plot with regression line (top row)\n",
        "    ax1 = axes[0, i]\n",
        "    ax1.scatter(x, y, color='blue')\n",
        "    ax1.plot(x, model.predict(X), color='red')\n",
        "    ax1.set_title(f'Dataset {i+1}')\n",
        "    ax1.set_xlabel('x')\n",
        "    ax1.set_ylabel('y')\n",
        "\n",
        "    # Residual plot (bottom row)\n",
        "    ax2 = axes[1, i]\n",
        "    ax2.scatter(x, model.resid, color='#ff6600')\n",
        "    ax2.axhline(0, color='gray', linestyle='--')\n",
        "    ax2.set_title(f'Residuals {i+1}')\n",
        "    ax2.set_xlabel('x')\n",
        "    ax2.set_ylabel('Residuals')\n",
        "\n",
        "# Show all plots\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Print regression summaries\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n=== Summary for Dataset {name} ===\")\n",
        "    print(model.summary())\n"
      ],
      "metadata": {
        "id": "oh1vtxEirU9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: for datasets III and IV compute leverages and Cook distances"
      ],
      "metadata": {
        "id": "zrNPnEKCrYrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework"
      ],
      "metadata": {
        "id": "NeAaWHbJrYYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if instead you model the ejection fraction as the dependent variable?\n",
        "\n",
        "Try to build the best linear regression model to predict ejection fraction"
      ],
      "metadata": {
        "id": "xEf9k9u2rd4b"
      }
    }
  ]
}